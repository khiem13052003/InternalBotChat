services:
  qdrant:
    build: ${HOST_VECTORDB_SERVICE_DIR}
    container_name: qdrant
    ports:
      - "6333:6333"   # REST API
      - "6334:6334"   # gRPC
    volumes:
      - qdrant_storage:/qdrant/storage
    # restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s


  mlc:
    image: dustynv/mlc:0.20.0-r36.4.0
    container_name: mlc
    runtime: nvidia
    # restart: unless-stopped
    # Ghi chú: privileged thường không cần, nhưng mlc image đôi khi yêu cầu.
    privileged: true
    ipc: host
    shm_size: '8g'
    env_file:
      - .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TZ=Asia/Ho_Chi_Minh
      - FORCE_CUDA=1
    volumes:
      - ${HOST_MODELS_DIR}:/mlc/models:ro
      - ${HOST_STORAGE_DIR}:/mlc/storage
      - rag_cache:/mlc/.cache
      - /etc/localtime:/etc/localtime:ro
    working_dir: /mlc
    ports:
      - "8000:8000"
    ulimits:
      memlock: -1
      nofile: 65536
    command:
      - mlc_llm 
      - serve 
      - ./models/llm/Llama-3.2-1B-q4f16_1-MLC
      - --host 
      - 0.0.0.0
      - --port 
      - "8000"
      - --overrides 
      - "max_num_sequence=1;max_total_seq_length=2048;gpu_memory_utilization=0.5;prefill_chunk_size=64;"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/v1/models || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s


  ingestion:
    build: ${HOST_INGESTION_SERVICE_DIR}
    container_name: ingestion
    depends_on:
      qdrant:
        condition: service_healthy
    env_file:
      - .env
    environment:
      TZ: Asia/Ho_Chi_Minh
      HF_TOKEN: ${HF_TOKEN}
      ORT_DISABLE_GPU: "1"
    volumes:
      - ${HOST_STORAGE_DIR}:/ingestion/storage
      - ${HOST_MODELS_DIR}:/ingestion/models
      - rag_cache:/ingestion/.cache
    command: ["python", "app.py"]
    # command: ["tail", "-f", "/dev/null"]
    # stdin_open: true
    # tty: true


  retrieval:
    env_file:
      - .env
    build: ${HOST_RETRIEVAL_SERVICE_DIR}
    container_name: retrieval
    ports:
      - "8001:8000"
    depends_on:
      qdrant:
        condition: service_healthy
    volumes:
      - ${HOST_STORAGE_DIR}:/retrieval/storage/
      - ${HOST_MODELS_DIR}:/retrieval/models
      - rag_cache:/retrieval/.cache
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s


  inference:
    env_file:
      - .env
    build: ${HOST_INFERENCE_SERVICE_DIR}
    container_name: inference
    ports:
      - "8002:8000"
    depends_on:
      mlc:
        condition: service_healthy
      retrieval:
        condition: service_healthy
    volumes:
      - rag_cache:/inference/.cache


volumes:
  rag_cache:
  qdrant_storage:
